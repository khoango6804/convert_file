import time
import os
import sys
import json
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from utils.log_config import setup_logging

from utils.doc_converter import extract_text_from_doc, extract_text_from_docx
from utils.file_utils import (
    ensure_folder_exists,
    move_to_error_folder,
    move_to_processed_folder,
    is_already_processed,
)
from utils.pdf_converter import PDFConverter
import logging


def setup_folders() -> Tuple[str, str, str, str]:  # Updated return type
    """Setup required folders and return their paths"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    folders = {
        "input": os.path.join(base_dir, "data", "input"),
        "output": os.path.join(base_dir, "data", "output"),
        "error": os.path.join(base_dir, "data", "error"),
        "processed": os.path.join(base_dir, "data", "processed"),
    }

    # Create folders if they don't exist
    for folder in folders.values():
        ensure_folder_exists(folder)

    return folders["input"], folders["output"], folders["error"], folders["processed"]


def check_input_files(
    input_folder: str, error_folder: str, output_folder: str, processed_folder: str
) -> Tuple[List[str], bool]:
    """Check input and error folders for valid files"""
    input_files = []
    error_files = []
    already_processed = []

    # Check input folder
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.lower().endswith((".doc", ".docx", ".pdf")):
                file_path = os.path.join(root, file)

                # Check if file has already been processed
                if is_already_processed(file_path, output_folder):
                    # Move directly to processed folder
                    logging.info(f"‚è≠Ô∏è ƒê√£ t√¨m th·∫•y file ƒë√£ x·ª≠ l√Ω: {file}")
                    move_to_processed_folder(file_path, processed_folder)
                    already_processed.append(file)
                else:
                    # New file, add to processing queue
                    input_files.append(file_path)

    # Check error folder
    for root, _, files in os.walk(error_folder):
        for file in files:
            if file.lower() != "error_log.txt" and file.lower().endswith(
                (".doc", ".docx", ".pdf")
            ):
                error_files.append(os.path.join(root, file))

    # Print summary of skipped files
    if already_processed:
        logging.info(
            f"‚è≠Ô∏è ƒê√£ chuy·ªÉn {len(already_processed)} file ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥ sang th∆∞ m·ª•c processed"
        )

    # If input is empty but error has files, process error folder
    if not input_files and error_files:
        logging.warning(
            f"‚ö†Ô∏è Th∆∞ m·ª•c input tr·ªëng. T√¨m th·∫•y {len(error_files)} file trong th∆∞ m·ª•c error."
        )
        logging.info("üîÑ Chuy·ªÉn sang x·ª≠ l√Ω files t·ª´ th∆∞ m·ª•c error...")
        return error_files, True

    # If input has files, process those
    if input_files:
        return input_files, False

    # If both are empty
    logging.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .doc, .docx ho·∫∑c .pdf n√†o ƒë·ªÉ x·ª≠ l√Ω")
    return [], False


def process_file(
    input_path, output_folder, error_folder, processed_folder, pdf_converter, log_file
):
    """Process a single file and handle errors"""
    file = os.path.basename(input_path)
    file_lower = file.lower()
    processed_file_paths = set()
    file_types = {"pdf": 0, "doc": 0, "docx": 0}
    processed_files = []
    error_files = []

    # Count file types
    if file_lower.endswith(".pdf"):
        file_types["pdf"] += 1
    elif file_lower.endswith(".docx"):
        file_types["docx"] += 1
    elif file_lower.endswith(".doc"):
        file_types["doc"] += 1

    try:
        # Skip temporary files
        if file_lower.startswith("~$"):
            return file_types, processed_files, error_files

        # Define output path
        output_path = os.path.join(output_folder, f"{os.path.splitext(file)[0]}.md")

        # Check for and remove existing original markdown file
        original_md_path = os.path.join(
            output_folder, f"{os.path.splitext(file)[0]}_original.md"
        )
        if os.path.exists(original_md_path):
            os.remove(original_md_path)

        # Process based on file type
        if file_lower.endswith(".pdf"):
            result = pdf_converter.pdf_to_text(input_path, output_format="md")
            if not result:
                raise Exception("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi PDF sang Markdown")
        else:
            # Handle DOC/DOCX
            extract_func = (
                extract_text_from_docx
                if file_lower.endswith(".docx")
                else extract_text_from_doc
            )
            text = extract_func(input_path)
            if text:
                # Write directly to final output file
                with open(output_path, "w", encoding="utf-8") as md_file:
                    md_file.write(text)
            else:
                raise Exception("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung")

        logging.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {file}")
        processed_files.append(file)

        # Move processed files to a processed folder
        move_to_processed_folder(input_path, processed_folder)

        if os.path.dirname(input_path) == error_folder:
            logging.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω file t·ª´ th∆∞ m·ª•c error: {file}")

    except Exception as e:
        error_msg = f"‚ùå L·ªói x·ª≠ l√Ω {file}: {str(e)}"
        logging.error(error_msg)

        # Append to log with timestamp
        with open(log_file, "a", encoding="utf-8") as log:
            log.write(f"## {time.strftime('%H:%M:%S')} - {file}\n")
            log.write(f"{str(e)}\n\n")

        move_to_error_folder(input_path, error_folder)
        if file not in error_files:
            error_files.append(file)

    return file_types, processed_files, error_files


def process_files(
    files: List[str], output_folder: str, error_folder: str, processed_folder: str
) -> Tuple[dict, List[str], List[str]]:
    """
    Process files and handle errors with visual progress tracking
    Returns:
        Tuple containing:
        - Dictionary of file types and counts
        - List of successfully processed files
        - List of error files
    """
    pdf_converter = PDFConverter()
    log_file = os.path.join(error_folder, "error_log.txt")
    file_types = {"pdf": 0, "doc": 0, "docx": 0}
    processed_files = []
    error_files = []

    # Clear previous log file content
    with open(log_file, "w", encoding="utf-8") as log:
        log.write(f"# Error Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

    # Kh·ªüi t·∫°o batch info file ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh
    batch_info_file = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), "data", "batch_info.json"
    )

    # L∆∞u th√¥ng tin batch khi b·∫Øt ƒë·∫ßu
    try:
        with open(batch_info_file, "w") as f:
            json.dump(
                {
                    "total_files": len(files),
                    "current_index": 0,
                    "start_time": time.time(),
                },
                f,
            )
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u th√¥ng tin batch: {str(e)}")

    try:
        total_files = len(files)
        logging.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {total_files} files...")

        # S·ª≠ d·ª•ng tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn ƒë·ªô v·ªõi ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=4) as executor:
            try:
                # T·∫°o t·∫•t c·∫£ c√°c future objects tr∆∞·ªõc
                futures = {
                    executor.submit(
                        process_file,
                        input_path,
                        output_folder,
                        error_folder,
                        processed_folder,
                        pdf_converter,
                        log_file,
                    ): input_path
                    for input_path in files
                }

                # S·ª≠ d·ª•ng tqdm c√πng v·ªõi logging
                progress_bar = tqdm(
                    total=len(futures),
                    desc="X·ª≠ l√Ω files",
                    unit="file",
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
                )

                for i, future in enumerate(as_completed(futures)):
                    input_path = futures[future]
                    filename = os.path.basename(input_path)
                    try:
                        # C·∫≠p nh·∫≠t th√¥ng tin batch
                        try:
                            with open(batch_info_file, "w") as f:
                                json.dump(
                                    {
                                        "total_files": total_files,
                                        "current_index": i,
                                        "current_file": filename,
                                        "processed": len(processed_files),
                                        "errors": len(error_files),
                                        "start_time": time.time(),
                                    },
                                    f,
                                )
                        except Exception:
                            pass

                        # L·∫•y k·∫øt qu·∫£ x·ª≠ l√Ω
                        file_type, processed, errors = future.result()

                        # C·∫≠p nh·∫≠t th·ªëng k√™
                        for key in file_types:
                            file_types[key] += file_type.get(key, 0)
                        processed_files.extend(processed)
                        error_files.extend(errors)

                        # C·∫≠p nh·∫≠t th√¥ng tin thanh ti·∫øn ƒë·ªô
                        progress_bar.set_postfix(
                            success=f"{len(processed_files)}/{i + 1}",
                            errors=len(error_files),
                        )
                        progress_bar.update(1)

                    except Exception as e:
                        logging.error(f"‚ùå L·ªói x·ª≠ l√Ω file {filename}: {str(e)}")
                        if filename not in error_files:
                            error_files.append(filename)

                        # Ghi l·ªói v√†o log
                        with open(log_file, "a", encoding="utf-8") as log:
                            log.write(f"## {time.strftime('%H:%M:%S')} - {filename}\n")
                            log.write(f"{str(e)}\n\n")

                        # Di chuy·ªÉn file l·ªói v√†o th∆∞ m·ª•c error
                        try:
                            move_to_error_folder(input_path, error_folder)
                        except Exception as move_err:
                            logging.error(
                                f"Kh√¥ng th·ªÉ di chuy·ªÉn file l·ªói: {str(move_err)}"
                            )

                        # V·∫´n c·∫≠p nh·∫≠t thanh ti·∫øn ƒë·ªô khi g·∫∑p l·ªói
                        progress_bar.update(1)
                progress_bar.close()

            except KeyboardInterrupt:
                # D·ª´ng c√°c futures ƒëang ch·∫°y khi c√≥ Ctrl+C
                progress_bar.close()
                logging.warning("\n‚ö†Ô∏è ƒêang d·ª´ng x·ª≠ l√Ω do ng∆∞·ªùi d√πng y√™u c·∫ßu (Ctrl+C)...")

                # L∆∞u th√¥ng tin batch cu·ªëi
                try:
                    with open(batch_info_file, "w") as f:
                        json.dump(
                            {
                                "total_files": total_files,
                                "current_index": i if "i" in locals() else 0,
                                "processed": len(processed_files),
                                "errors": len(error_files),
                                "interrupted": True,
                                "last_update": time.time(),
                            },
                            f,
                        )
                except Exception:
                    pass

                logging.info(
                    "üíæ ƒê√£ l∆∞u th√¥ng tin x·ª≠ l√Ω. Qu√° tr√¨nh x·ª≠ l√Ω PDF ƒëang th·ª±c hi·ªán s·∫Ω t·ª± l∆∞u ti·∫øn ƒë·ªô."
                )

                # Cancel remaining futures
                for fut in futures:
                    if not fut.done() and not fut.cancelled():
                        fut.cancel()

                # ƒê·ª£i c√°c futures ƒëang ch·∫°y ho√†n th√†nh ho·∫∑c b·ªã cancel
                # L∆∞u √Ω: kh√¥ng th·ªÉ cancel futures ƒëang ch·∫°y, ch·ªâ c√°c futures ch∆∞a b·∫Øt ƒë·∫ßu
                logging.info("‚åõ ƒêang ƒë·ª£i c√°c ti·∫øn tr√¨nh hi·ªán t·∫°i ho√†n t·∫•t...")
                time.sleep(3)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
        success_rate = (
            len(processed_files) / total_files * 100 if total_files > 0 else 0
        )
        logging.info(
            f"‚úÖ Ho√†n th√†nh: {len(processed_files)}/{total_files} files ({success_rate:.1f}%)"
        )

        if error_files:
            logging.warning(
                f"‚ö†Ô∏è C√≥ {len(error_files)} files l·ªói, xem chi ti·∫øt trong log"
            )
        logging.info("‚åõ ƒêang ƒë·ª£i c√°c ti·∫øn tr√¨nh hi·ªán t·∫°i ho√†n t·∫•t...")
        time.sleep(3)

    except Exception as e:
        logging.error(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω files: {str(e)}")

    finally:
        # Make sure to cleanup
        try:
            pdf_converter.cleanup()
        except Exception:
            pass

        return file_types, processed_files, error_files


def create_api_json():
    """Create or update api.json file with Google API key"""
    try:
        # Get base directory
        base_dir = os.path.dirname(os.path.abspath(__file__))
        api_file = os.path.join(base_dir, "api.json")

        # Check if api.json exists and is valid
        if os.path.exists(api_file):
            try:
                with open(api_file, "r", encoding="utf-8") as f:
                    existing_config = json.load(f)
                    if existing_config.get("api_key"):
                        logging.info("\n‚úÖ ƒê√£ t√¨m th·∫•y API key")
                        use_existing = (
                            input("B·∫°n c√≥ mu·ªën s·ª≠ d·ª•ng API key hi·ªán t·∫°i? (y/n): ")
                            .lower()
                            .strip()
                        )
                        if use_existing == "y":
                            return True
            except json.JSONDecodeError:
                logging.warning("\n‚ö†Ô∏è File api.json hi·ªán t·∫°i kh√¥ng h·ª£p l·ªá. T·∫°o m·ªõi...")
            except Exception as e:
                logging.warning(f"\n‚ö†Ô∏è L·ªói ƒë·ªçc file api.json: {str(e)}")

        # Get API key from user
        logging.info("\nüìù C√†i ƒë·∫∑t Google API Key")
        logging.info("1. Truy c·∫≠p: https://makersuite.google.com/app/apikey")
        logging.info("2. T·∫°o API key m·ªõi ho·∫∑c s·ª≠ d·ª•ng key c√≥ s·∫µn")
        api_key = input("\nüîë Nh·∫≠p API key c·ªßa b·∫°n: ").strip()

        if not api_key:
            logging.error("‚ùå API key kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
            return False

        # Create API configuration
        config = {"api_key": api_key}

        # Write to file
        with open(api_file, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=4, ensure_ascii=False)

        logging.info(f"\n‚úÖ ƒê√£ l∆∞u API key v√†o: {api_file}")
        return True

    except Exception as e:
        logging.error(f"\n‚ùå L·ªói t·∫°o file api.json: {str(e)}")
        return False


def main():
    """Main execution flow"""
    try:
        # Setup logging
        base_dir = os.path.dirname(os.path.abspath(__file__))
        log_folder = os.path.join(base_dir, "data", "logs")
        logger = setup_logging(log_folder)

        logging.info("üöÄ Ch∆∞∆°ng tr√¨nh b·∫Øt ƒë·∫ßu")

        # 0. Check API configuration
        if not os.path.exists("api.json"):
            logging.warning("‚ö†Ô∏è Ch∆∞a c√≥ file c·∫•u h√¨nh API")
            if not create_api_json():
                logging.error("‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c khi ch∆∞a c√≥ API key")
                return 1

        # Rest of your main function with logging instead of print
        logging.info("üìÇ ƒêang t·∫°o th∆∞ m·ª•c...")
        input_folder, output_folder, error_folder, processed_folder = setup_folders()
        logging.info("‚úÖ ƒê√£ t·∫°o xong c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt")

        # 1. Setup folders
        logging.info("\nüìÇ ƒêang t·∫°o th∆∞ m·ª•c...")
        input_folder, output_folder, error_folder, processed_folder = setup_folders()
        logging.info("‚úÖ ƒê√£ t·∫°o xong c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt")

        # 2. Check input files
        logging.info("\nüîç ƒêang ki·ªÉm tra file...")
        # Fix this line to pass all required arguments
        valid_files, is_error_processing = check_input_files(
            input_folder, error_folder, output_folder, processed_folder
        )
        if not valid_files:
            logging.error("‚ùå Kh√¥ng t√¨m th·∫•y file n√†o ƒë·ªÉ x·ª≠ l√Ω")
            logging.info(f"‚ÑπÔ∏è Vui l√≤ng th√™m file v√†o th∆∞ m·ª•c: {input_folder}")
            return 1

        # 3. Process files
        source_folder = error_folder if is_error_processing else input_folder
        total_files = len(valid_files)
        logging.info(f"\nüîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {total_files} files t·ª´ {source_folder}...")

        # Get file statistics from process_files
        # You can either modify process_files to accept processed_folder or keep using the
        # hardcoded path in the process_files function
        file_types, processed_files, error_files = process_files(
            valid_files, output_folder, error_folder, processed_folder
        )

        logging.info("\n‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω files")

        # Read error log and store details
        error_details = {}
        error_log_path = os.path.join(error_folder, "error_log.txt")
        if os.path.exists(error_log_path):
            with open(error_log_path, "r", encoding="utf-8") as log:
                for line in log:
                    if "‚ùå L·ªói x·ª≠ l√Ω" in line:
                        file_name = line.split("‚ùå L·ªói x·ª≠ l√Ω")[1].split(":")[0].strip()
                        error_msg = line.split(":", 1)[1].strip()
                        error_details[file_name] = error_msg

        # Print statistics
        logging.info("\nüìÅ T·ªîNG QUAN")
        logging.info(f"- Ngu·ªìn x·ª≠ l√Ω: {source_folder}")
        logging.info(f"- T·ªïng s·ªë file: {total_files}")

        logging.info("\nüìä PH√ÇN LO·∫†I")
        logging.info("- ƒê·ªãnh d·∫°ng file:")
        for ext, count in file_types.items():
            if count > 0:
                logging.info(f"  ‚Ä¢ {ext.upper()}: {count} files")

        print("\nüìà K·∫æT QU·∫¢")
        success_count = len(processed_files)
        error_count = len(error_files)
        print(
            f"- Th√†nh c√¥ng: {success_count} files ({success_count / total_files * 100:.1f}%)"
        )
        print(
            f"- Th·∫•t b·∫°i: {error_count} files ({error_count / total_files * 100:.1f}%)"
        )

        if error_files:
            print("\n‚ùå CHI TI·∫æT L·ªñI")
            for file in error_files:
                error_msg = error_details.get(file, "Kh√¥ng c√≥ th√¥ng tin l·ªói")
                print(f"- {file}")
                print(f"  ‚Üí {error_msg}")

        print("\n" + "=" * 50)
        if error_count > 0:
            logging.warning(f"‚ö†Ô∏è Xem chi ti·∫øt l·ªói trong: {error_log_path}")
            logging.warning(f"üìÅ Files l·ªói ƒë∆∞·ª£c chuy·ªÉn v√†o: {error_folder}")
        else:
            logging.info("‚úÖ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng!")

        return 0

    except Exception as e:
        logging.error(f"\n‚ùå L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {str(e)}")
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
    except KeyboardInterrupt:
        logging.info("\n‚ö†Ô∏è ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh.")
        exit_code = 0
    except Exception as e:
        logging.error(f"\n‚ùå L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {str(e)}")
        exit_code = 1
    finally:
        sys.exit(exit_code)
