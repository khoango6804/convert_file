import io
import os
import fitz
from PIL import Image, ImageEnhance, ImageOps
import numpy as np
from google import genai
import tempfile
import json
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from datetime import datetime, timedelta

import logging
from tqdm import tqdm


def estimate_tokens(self, text=None, image=None):
    """Estimate token usage of a request"""
    token_count = 0

    # Text token estimation (approx 4 chars per token)
    if text:
        token_count += len(text) / 4

    # Image token estimation based on dimensions
    if image and hasattr(image, "width") and hasattr(image, "height"):
        # Gemini charges more for larger images
        pixels = image.width * image.height
        token_count += min(pixels / 750, 4000)  # Rough estimate

    return int(token_count)


class PDFConverter:
    def __init__(self):
        # Load API key from config
        try:
            with open(
                os.path.join(os.path.dirname(os.path.dirname(__file__)), "api.json"),
                "r",
            ) as f:
                config = json.load(f)

                # Support both single key and multiple keys formats
                if "api_keys" in config:
                    self.api_keys = config["api_keys"]
                    self.single_key = False
                elif "api_key" in config:
                    self.api_keys = [config["api_key"]]
                    self.single_key = True
                else:
                    raise ValueError("No API keys found in config")

                # Use the first key initially
                self.current_key_index = 0

                # Configure Gemini
                self.client = genai.Client(
                    api_key=self.api_keys[self.current_key_index]
                )
                logging.info(
                    f"‚úÖ ƒê√£ k·∫øt n·ªëi Gemini API (Key {self.current_key_index + 1}/{len(self.api_keys)})"
                )

                # Initialize rate limiting tracking for each key
                # Gemini limits: 15 RPM, 1M TPM, 1,500 RPD
                self.key_usage = {}
                for i, _ in enumerate(self.api_keys):
                    self.key_usage[i] = {
                        "minute_requests": [],  # List of timestamps for RPM tracking
                        "day_requests": 0,  # Counter for RPD tracking
                        "day_reset": time.time() + 86400,  # Next day reset time
                        "tokens_used_minute": 0,  # Tokens used in current minute
                        "minute_reset": time.time() + 60,  # Next minute reset time
                    }

        except Exception as e:
            logging.error(f"‚ùå L·ªói c·∫•u h√¨nh Gemini API: {e}")
            raise

        self.temp_dir = tempfile.mkdtemp()
        self.output_dir = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "data", "output"
        )
        self.retry_count = 0
        self.max_retries = 3
        self.wait_time = 60  # seconds

        # Rate limits for Gemini API
        self.RPM_LIMIT = 15  # Requests per minute
        self.TPM_LIMIT = 1000000  # Tokens per minute
        self.RPD_LIMIT = 1500  # Requests per day

        # Track rotation to prevent endless cycling
        self._rotation_cycle_count = 0
        self._last_rotation_time = time.time()

    def _update_rate_limits(self, estimated_tokens=0):
        """Update rate limit tracking for current key"""
        key_index = self.current_key_index
        current_time = time.time()

        # Update minute-based tracking
        self.key_usage[key_index]["minute_requests"].append(current_time)
        self.key_usage[key_index]["tokens_used_minute"] += estimated_tokens

        # Remove timestamps older than 1 minute
        self.key_usage[key_index]["minute_requests"] = [
            t
            for t in self.key_usage[key_index]["minute_requests"]
            if t > current_time - 60
        ]

        # Check if minute reset time passed
        if current_time > self.key_usage[key_index]["minute_reset"]:
            self.key_usage[key_index]["tokens_used_minute"] = estimated_tokens
            self.key_usage[key_index]["minute_reset"] = current_time + 60
            logging.info(f"üîÑ Reset minute-based limits for API key {key_index + 1}")

        # Update day-based tracking
        self.key_usage[key_index]["day_requests"] += 1

        # Check if day reset time passed
        if current_time > self.key_usage[key_index]["day_reset"]:
            self.key_usage[key_index]["day_requests"] = 1
            self.key_usage[key_index]["day_reset"] = current_time + 86400
            logging.info(f"üîÑ Reset daily limits for API key {key_index + 1}")

    def _check_rate_limits(self):
        """Check if current key is within rate limits"""
        key_index = self.current_key_index
        current_usage = self.key_usage[key_index]

        # Check RPM limit
        rpm_current = len(current_usage["minute_requests"])
        if rpm_current >= self.RPM_LIMIT:
            logging.warning(
                f"‚ö†Ô∏è API key {key_index + 1} ƒë·∫°t gi·ªõi h·∫°n RPM ({rpm_current}/{self.RPM_LIMIT})"
            )
            return False

        # Check TPM limit
        if current_usage["tokens_used_minute"] >= self.TPM_LIMIT:
            logging.warning(
                f"‚ö†Ô∏è API key {key_index + 1} ƒë·∫°t gi·ªõi h·∫°n TPM ({current_usage['tokens_used_minute']}/{self.TPM_LIMIT})"
            )
            return False

        # Check RPD limit
        if current_usage["day_requests"] >= self.RPD_LIMIT:
            logging.warning(
                f"‚ö†Ô∏è API key {key_index + 1} ƒë·∫°t gi·ªõi h·∫°n RPD ({current_usage['day_requests']}/{self.RPD_LIMIT})"
            )
            return False

        return True

    def _find_available_key(self):
        """Find an API key that hasn't reached its rate limits"""
        original_key = self.current_key_index

        # Try each key once
        for _ in range(len(self.api_keys)):
            # Check if current key is within limits
            if self._check_rate_limits():
                return True

            # Rotate to next key
            self.rotate_api_key()

            # If we've tried all keys and came back to the original one
            if self.current_key_index == original_key:
                break

        # If all keys are rate-limited
        wait_time = self._calculate_min_wait_time()
        logging.error(
            f"‚ùå All API keys are rate-limited. Wait at least {wait_time} seconds."
        )
        return False

    def _calculate_min_wait_time(self):
        """Calculate minimum time to wait for any key to become available again"""
        current_time = time.time()
        min_wait = 60  # Default 1 minute

        for key_index in self.key_usage:
            usage = self.key_usage[key_index]

            # Check minute-based reset
            if len(usage["minute_requests"]) >= self.RPM_LIMIT:
                # Find oldest request timestamp
                oldest = min(usage["minute_requests"])
                # Time until this falls outside the 1-minute window
                wait_time = (oldest + 60) - current_time
                min_wait = min(min_wait, max(1, wait_time))

            # If we're at TPM limit, we need to wait until minute reset
            if usage["tokens_used_minute"] >= self.TPM_LIMIT:
                wait_time = usage["minute_reset"] - current_time
                min_wait = min(min_wait, max(1, wait_time))

        return int(min_wait)

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _call_gemini_api(self, prompt, image=None):
        """Simplified API call with error handling and key rotation"""
        try:
            # Reset cycle detection if enough time has passed
            current_time = time.time()
            if (current_time - getattr(self, "_last_reset_time", 0)) > 60:
                if hasattr(self, "rotation_count"):
                    self.rotation_count = 0
                if hasattr(self, "_tried_all_keys"):
                    self._tried_all_keys = False
                if hasattr(self, "_exhausted_keys"):
                    self._exhausted_keys = set()
                self._last_reset_time = current_time

            # Check if we're cycling keys too rapidly
            if hasattr(self, "rotation_count") and self.rotation_count >= len(
                self.api_keys
            ):
                logging.warning(
                    "‚ö†Ô∏è ƒê√£ th·ª≠ t·∫•t c·∫£ API key trong th·ªùi gian ng·∫Øn. T·∫°m d·ª´ng 10 gi√¢y..."
                )
                time.sleep(10)  # Take a shorter break before retrying
                self.rotation_count = 0

            # Check if current key is within rate limits, if not find one that is
            if not self._check_rate_limits():
                if not self._find_available_key():
                    wait_time = self._calculate_min_wait_time()
                    logging.warning(f"‚è±Ô∏è ƒêang ƒë·ª£i {wait_time} gi√¢y cho API key reset...")
                    time.sleep(wait_time)

            # Estimate token usage for tracking
            prompt_tokens = self.estimate_tokens(text=prompt)
            image_tokens = 0
            if image:
                if isinstance(image, Image.Image):
                    image_tokens = self.estimate_tokens(image=image)
                else:
                    image_tokens = 1000  # Rough estimate for image bytes

            estimated_tokens = prompt_tokens + image_tokens

            # Make API call
            try:
                model = "gemini-2.0-flash"

                # Make API call with selected model
                if image:
                    # Check if the image is a PIL Image object and convert to bytes if needed
                    if isinstance(image, Image.Image):
                        # Convert PIL Image to bytes
                        img_bytes = io.BytesIO()
                        image.save(img_bytes, format="JPEG", quality=85)
                        img_bytes = img_bytes.getvalue()
                    else:
                        # Assume it's already bytes
                        img_bytes = image

                    # Create a temporary file for the image
                    with tempfile.NamedTemporaryFile(
                        suffix=".jpg", delete=False
                    ) as temp_img:
                        temp_img.write(img_bytes)
                        temp_img_path = temp_img.name

                    try:
                        # Upload the image file to Gemini
                        file_ref = self.client.files.upload(file=temp_img_path)

                        # Call Gemini with text prompt and file reference
                        response = self.client.models.generate_content(
                            model=model, contents=[prompt, file_ref]
                        )
                    finally:
                        # Clean up the temporary file
                        try:
                            os.unlink(temp_img_path)
                        except:
                            pass
                else:
                    # Text-only request
                    response = self.client.models.generate_content(
                        model=model, contents=[prompt]
                    )

                # Success - update rate limit tracking
                self._update_rate_limits(estimated_tokens)

                # Reset rotation counter and tried_all_keys flag
                if hasattr(self, "rotation_count"):
                    self.rotation_count = 0
                if hasattr(self, "_tried_all_keys"):
                    self._tried_all_keys = False
                if hasattr(self, "_exhausted_keys"):
                    self._exhausted_keys.clear()

                return response

            except Exception as e:
                error_message = str(e).lower()
                error_code = None

                # Extract error code if present
                if "429" in error_message:
                    error_code = 429
                elif "'code': " in error_message:
                    try:
                        code_str = (
                            error_message.split("'code': ")[1].split(",")[0].strip()
                        )
                        if code_str.isdigit():
                            error_code = int(code_str)
                    except:
                        pass

                # Check if this is a rate limit/resource exhausted error
                is_rate_limit_error = (
                    error_code == 429
                    or "resource exhausted" in error_message
                    or "rate limit" in error_message
                    or "too many requests" in error_message
                )

                # Check if this is a quota error (different from rate limit)
                is_quota_error = (
                    "quota exceeded" in error_message or "usage limit" in error_message
                )

                # Handle resource exhaustion by adding delay before retrying
                if is_rate_limit_error:
                    logging.warning(
                        f"‚ö†Ô∏è API key {self.current_key_index + 1}/{len(self.api_keys)} ƒëang b·ªã gi·ªõi h·∫°n t·∫°m th·ªùi (rate limit)"
                    )

                    # Add a delay to recover from rate limiting
                    delay = 5 + (
                        self.retry_count * 5
                    )  # Increasing delay with each retry
                    logging.info(f"‚è±Ô∏è ƒê·ª£i {delay} gi√¢y cho rate limit reset...")
                    time.sleep(delay)

                    # Try with a different key if we have multiple keys
                    if len(self.api_keys) > 1:
                        # Try rotating to a new key
                        previous_key = self.current_key_index
                        self.rotate_api_key()
                        logging.info(
                            f"üîÑ ƒê·ªïi t·ª´ API key {previous_key + 1} sang {self.current_key_index + 1}/{len(self.api_keys)} do rate limit"
                        )
                        return self._call_gemini_api(prompt, image)

                    # For single key, just raise to let tenacity retry
                    raise

                # Handle permanent quota exhaustion
                elif is_quota_error:
                    # Update tracking for this key to mark it as exhausted for today
                    self.key_usage[self.current_key_index]["day_requests"] = (
                        self.RPD_LIMIT
                    )

                    logging.warning(
                        f"‚ö†Ô∏è API key {self.current_key_index + 1}/{len(self.api_keys)} ƒë√£ h·∫øt quota."
                    )

                    # Initialize exhausted keys tracking if needed
                    if not hasattr(self, "_exhausted_keys"):
                        self._exhausted_keys = set()

                    # Mark current key as tried
                    self._exhausted_keys.add(self.current_key_index)

                    # If we have multiple keys, try to find an unused one
                    if len(self.api_keys) > 1:
                        # Try each key at most once
                        for _ in range(len(self.api_keys) - 1):
                            self.rotate_api_key()

                            # Skip this key if we've already tried it
                            if self.current_key_index in self._exhausted_keys:
                                continue

                            logging.info(
                                f"üîÑ ƒê√£ chuy·ªÉn sang API key {self.current_key_index + 1}/{len(self.api_keys)} do h·∫øt quota"
                            )
                            return self._call_gemini_api(prompt, image)

                        # If we've tried all keys during this attempt
                        if len(self._exhausted_keys) >= len(self.api_keys):
                            logging.error(
                                f"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(self.api_keys)} API keys. C√≥ th·ªÉ t·∫•t c·∫£ ƒë·ªÅu h·∫øt quota."
                            )
                            raise Exception("ALL_KEYS_EXHAUSTED")
                    else:
                        # Only one key and it's exhausted
                        logging.error("‚ùå API key duy nh·∫•t ƒë√£ h·∫øt quota.")
                        raise Exception("API_QUOTA_EXHAUSTED")
                else:
                    # Handle other errors - log the full error for debugging
                    logging.warning(f"‚ö†Ô∏è API error (non-quota): {str(e)}")

                    # Add a small delay for other errors
                    time.sleep(1)
                    raise

        except Exception as e:
            if "ALL_KEYS_EXHAUSTED" in str(e) or "API_QUOTA_EXHAUSTED" in str(e):
                # Forward these specific errors to calling function
                raise

            logging.error(f"‚ùå API error: {str(e)}")

            # If we've retried too many times, take a break
            if self.retry_count >= self.max_retries:
                logging.error(
                    f"‚ö†Ô∏è Maximum retries reached. Waiting {self.wait_time} seconds..."
                )
                time.sleep(self.wait_time)
                self.retry_count = 0
            else:
                self.retry_count += 1

            raise

    def _adjust_quota_tracker(self):
        """Adjust quota tracker to match current API keys"""
        current_keys = self.quota_tracker["keys"].copy()
        new_keys = []

        # Keep existing data for keys we already have
        for i in range(len(self.api_keys)):
            if i < len(current_keys):
                new_keys.append(current_keys[i])
            else:
                # Add new entries for additional keys
                tomorrow = (
                    datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                    + timedelta(days=1)
                ).isoformat()

                new_keys.append({"daily_requests": 0, "next_allowed_time": tomorrow})

        # Update the tracker
        self.quota_tracker["keys"] = new_keys
        logging.info(f"‚úÖ ƒê√£ ƒëi·ªÅu ch·ªânh quota tracker cho {len(new_keys)} API keys")

    def save_quota_tracker(self):
        """Save quota tracker to file"""
        with open(self.quota_tracker_file, "w") as f:
            json.dump(self.quota_tracker, f, indent=2)

    def update_key_usage(self):
        """Update usage for current key and check if we need to wait"""
        key_data = self.quota_tracker["keys"][self.current_key_index]
        key_data["daily_requests"] += 1

        # If we've reached the daily limit (1500 for free tier)
        if key_data["daily_requests"] >= 1500:
            # Set next allowed time to 24 hours from now
            next_time = datetime.now() + timedelta(hours=24)
            key_data["next_allowed_time"] = next_time.isoformat()
            logging.warning(
                f"‚ö†Ô∏è Daily limit reached for key {self.current_key_index + 1}. Next allowed: {next_time}"
            )

        self.save_quota_tracker()
        return key_data["daily_requests"] >= 1500

    def check_key_availability(self):
        """Check if current key is available or if we need to wait/rotate"""
        key_data = self.quota_tracker["keys"][self.current_key_index]
        next_allowed_time = datetime.fromisoformat(key_data["next_allowed_time"])

        if datetime.now() < next_allowed_time:
            # Key is not available yet
            time_diff = next_allowed_time - datetime.now()
            hours, remainder = divmod(time_diff.seconds, 3600)
            minutes, seconds = divmod(remainder, 60)

            wait_message = f"‚ö†Ô∏è API key {self.current_key_index + 1} ƒëang b·ªã gi·ªõi h·∫°n. "
            wait_message += f"Ti·∫øp t·ª•c sau: {hours} gi·ªù, {minutes} ph√∫t, {seconds} gi√¢y"

            if len(self.api_keys) > 1:
                # Try other keys
                original_key = self.current_key_index
                for _ in range(len(self.api_keys) - 1):
                    self.rotate_api_key()
                    if self.check_key_availability():
                        logging.info(
                            f"üîÑ ƒê√£ chuy·ªÉn sang API key {self.current_key_index + 1} v√¨ key {original_key + 1} ƒëang b·ªã gi·ªõi h·∫°n"
                        )
                        return True

                # If we're here, all keys are exhausted
                logging.error(wait_message)
                return False
            else:
                # We only have one key and it's exhausted
                logging.error(wait_message)
                return False

        return True  # Key is available

    def rotate_api_key(self):
        """Simple API key rotation without quota tracking"""
        if len(self.api_keys) <= 1:
            return False  # Can't rotate with only one key

        # Store previous key index for logging
        previous_key = self.current_key_index

        # Move to next key
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)

        # Configure client with new key
        try:
            self.client = genai.Client(api_key=self.api_keys[self.current_key_index])

            # Track rotation time
            self._last_rotation_time = time.time()

            # Track rotations
            if not hasattr(self, "rotation_count"):
                self.rotation_count = 0
            self.rotation_count += 1

            logging.info(
                f"üîÑ ƒê√£ chuy·ªÉn sang API key {self.current_key_index + 1}/{len(self.api_keys)}"
            )

            return True
        except Exception as e:
            logging.error(f"‚ùå L·ªói khi chuy·ªÉn API key: {str(e)}")
            self.current_key_index = previous_key  # Revert to previous key
            return False

    def preprocess_image(self, image):
        """
        Preprocess image to improve OCR quality
        Args:
            image: PIL Image object
        Returns:
            Preprocessed PIL Image object
        """
        try:
            # 1. Convert to grayscale
            img_gray = ImageOps.grayscale(image)

            # 2. Increase contrast
            enhancer = ImageEnhance.Contrast(img_gray)
            img_contrast = enhancer.enhance(2.0)  # Increase contrast by factor of 2

            # 3. Binarization using Otsu's method
            img_array = np.array(img_contrast)
            threshold = int(np.mean(img_array))
            img_binary = Image.fromarray((img_array > threshold).astype(np.uint8) * 255)

            # 4. Denoise
            img_denoised = ImageOps.autocontrast(img_binary, cutoff=1)

            # 5. Remove borders
            bbox = ImageOps.invert(img_denoised).getbbox()
            if bbox:
                img_cropped = img_denoised.crop(bbox)
            else:
                img_cropped = img_denoised

            logging.info("‚ú® ƒê√£ x·ª≠ l√Ω ·∫£nh ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng OCR")
            return img_cropped

        except Exception as e:
            logging.warning(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
            return image

    def check_vietnamese_text(self, text: str) -> str:
        """
        Validate and correct Vietnamese text using Gemini
        """
        try:
            prompt = """
            Chu·∫©n h√≥a vƒÉn b·∫£n h√†nh ch√≠nh th√†nh Markdown:

            1. Ti√™u ƒë·ªÅ:
            - # cho c∆° quan cao nh·∫•t 
            - ## cho c∆° quan tr·ª±c thu·ªôc
            - ### cho s·ªë vƒÉn b·∫£n/tr√≠ch y·∫øu

            2. ƒê·ªãnh d·∫°ng:
            - **C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM**
            - *ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c*
            - Xu·ªëng d√≤ng: 2 d·∫•u c√°ch cu·ªëi d√≤ng
            - CƒÉn ph·∫£i: 4 d·∫•u c√°ch ƒë·∫ßu d√≤ng

            3. Danh s√°ch:
            - C√≥ d√≤ng tr·ªëng tr∆∞·ªõc/sau
            - - danh s√°ch kh√¥ng th·ª© t·ª±
            - 1. 2. danh s√°ch c√≥ th·ª© t·ª±
            - > cho tr√≠ch d·∫´n

            4. B·∫£ng v√† ghi ch√∫:
            - | v√† - cho b·∫£ng, :--- cƒÉn tr√°i, ---: cƒÉn ph·∫£i
            - Ghi ch√∫ d·∫°ng [^n] thay cho <sup>n</sup>

            Kh√¥ng d√πng HTML, kh√¥ng ƒë·ªÉ d·∫•u c√¢u cu·ªëi ti√™u ƒë·ªÅ, th√™m d√≤ng tr·ªëng cu·ªëi file.
            """

            # Get response from Gemini
            response = self.client.models.generate_content(
                model="gemini-2.0-flash", contents=[prompt, text]
            )

            return response.text if response.text else text

        except Exception as e:
            logging.warning(f"‚ö†Ô∏è L·ªói ki·ªÉm tra ch√≠nh t·∫£: {e}")
            return text

    def clean_markdown_content(self, text: str) -> str:
        """
        Clean markdown content and normalize spacing while preserving content structure
        """
        try:
            lines = text.split("\n")
            cleaned_lines = []
            prev_line_empty = False

            for line in lines:
                # Remove ``` markers
                if line.strip().startswith("```"):
                    line = line.replace("```markdown", "").replace("```", "").strip()

                # Skip if line is empty after cleaning
                if not line.strip():
                    if not prev_line_empty:  # Only add one empty line
                        cleaned_lines.append("")
                        prev_line_empty = True
                    continue

                # Handle headings (add space after #)
                if line.strip().startswith("#"):
                    line = line.strip()
                    parts = line.split(" ", 1)
                    if len(parts) > 1:
                        line = f"{parts[0]} {parts[1].strip()}"

                # Handle lists (preserve indentation)
                elif line.strip().startswith(("-", "*", "1.")):
                    indent = len(line) - len(line.lstrip())
                    line = " " * indent + line.strip()

                # Handle tables (normalize spacing)
                elif "|" in line:
                    cells = [cell.strip() for cell in line.split("|")]
                    line = "|".join(cells)

                # Normal lines
                else:
                    line = line.strip()

                cleaned_lines.append(line)
                prev_line_empty = False

            # Ensure single newline at end
            cleaned_text = "\n".join(cleaned_lines).strip() + "\n"
            return cleaned_text

        except Exception as e:
            logging.warning(f"‚ö†Ô∏è L·ªói khi l√†m s·∫°ch Markdown: {e}")
            return text

    def estimate_tokens(self, text=None, image=None):
        """Estimate token usage of a request"""
        token_count = 0

        # Text token estimation (approx 4 chars per token)
        if text:
            token_count += len(text) / 4

        # Image token estimation based on dimensions
        if image and hasattr(image, "width") and hasattr(image, "height"):
            # Gemini charges more for larger images
            pixels = image.width * image.height
            token_count += min(pixels / 750, 4000)  # Rough estimate

        return int(token_count)

    def pdf_to_text(
        self, pdf_path, output_format="md", token_budget=None, resume=False
    ):
        """
        Convert PDF with optional token budget
        Args:
            token_budget: Maximum tokens to use (None for unlimited)
        """
        estimated_tokens_used = 0
        pdf_document = None
        start_time = time.time()

        # Define progress file path based on PDF name
        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
        progress_dir = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "data", "progress"
        )
        os.makedirs(progress_dir, exist_ok=True)
        progress_file = os.path.join(progress_dir, f"{pdf_filename}_progress.json")

        # Initialize or load progress data
        processed_pages = {}
        current_page_retries = {}
        start_page = 0

        if resume and os.path.exists(progress_file):
            try:
                with open(progress_file, "r", encoding="utf-8") as f:
                    progress_data = json.load(f)
                    processed_pages = progress_data.get("pages", {})
                    estimated_tokens_used = progress_data.get("tokens_used", 0)
                    current_page_retries = progress_data.get("page_retries", {})
                    logging.info(
                        f"üîÑ Ti·∫øp t·ª•c x·ª≠ l√Ω t·ª´ ti·∫øn ƒë·ªô ƒë√£ l∆∞u. ƒê√£ x·ª≠ l√Ω {len(processed_pages)} trang."
                    )
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i ti·∫øn ƒë·ªô ƒë√£ l∆∞u: {str(e)}")

        try:
            # Open PDF
            pdf_document = fitz.open(pdf_path)

            if pdf_document.page_count == 0:
                logging.warning(
                    f"‚ö†Ô∏è PDF kh√¥ng c√≥ trang n√†o: {os.path.basename(pdf_path)}"
                )
                return False

            all_text = []
            logging.info(f"üîç ƒêang chuy·ªÉn ƒë·ªïi {pdf_path}...")

            # Tracking variables
            success_count = len(processed_pages)
            failed_count = 0
            total_pages = pdf_document.page_count

            # Hi·ªÉn th·ªã th√¥ng tin PDF
            logging.info(
                f"üìÑ File PDF c√≥ {total_pages} trang, ƒë√£ x·ª≠ l√Ω {success_count} trang"
            )

            # Kh·ªüi t·∫°o thanh ti·∫øn ƒë·ªô v·ªõi tqdm
            pbar = tqdm(
                total=total_pages,
                initial=success_count,
                desc="X·ª≠ l√Ω trang PDF",
                unit="trang",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
            )
            try:
                for page_num in range(total_pages):
                    # Skip already processed pages if resuming
                    page_key = str(page_num)
                    if resume and page_key in processed_pages:
                        all_text.append(processed_pages[page_key])
                        continue

                    retry_count = current_page_retries.get(page_key, 0)
                    max_page_retries = 2
                    page_processed = False

                    while retry_count <= max_page_retries and not page_processed:
                        try:
                            # Get the page with error handling
                            try:
                                page = pdf_document[page_num]
                            except IndexError:
                                logging.warning(
                                    f"‚ö†Ô∏è L·ªói khi truy c·∫≠p trang {page_num + 1}"
                                )
                                failed_count += 1
                                break

                            if not page:
                                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc trang {page_num + 1}")
                                failed_count += 1
                                break

                            # Convert page to image
                            try:
                                pix = page.get_pixmap(
                                    matrix=fitz.Matrix(200 / 72, 200 / 72)
                                )
                                img = Image.frombytes(
                                    "RGB", [pix.width, pix.height], pix.samples
                                )
                            except Exception as img_err:
                                logging.warning(
                                    f"‚ö†Ô∏è L·ªói khi t·∫°o ·∫£nh trang {page_num + 1}: {str(img_err)}"
                                )
                                retry_count += 1
                                continue

                            # Preprocess image
                            img_processed = self.preprocess_image(img)

                            # S·ª≠ d·ª•ng prompt ng·∫Øn h∆°n ƒë·ªÉ ti·∫øt ki·ªám token
                            prompt = """
                            Chuy·ªÉn th√†nh Markdown:
                            - # ti√™u ƒë·ªÅ ch√≠nh, ## ti√™u ƒë·ªÅ c·∫•p 2, ### ph·∫ßn ch√≠nh
                            - **C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM**
                            - *Th·ªùi gian, ƒë·ªãa ƒëi·ªÉm*
                            - > tr√≠ch d·∫´n, --- ƒë∆∞·ªùng k·∫ª
                            - | v√† - cho b·∫£ng (s·ªë cƒÉn ph·∫£i)
                            - - danh s√°ch kh√¥ng th·ª© t·ª±, 1. danh s√°ch th·ª© t·ª±
                            Gi·ªØ nguy√™n m√£ vƒÉn b·∫£n, s·ªë li·ªáu, d·∫•u c√¢u, ƒë·ªãnh d·∫°ng ti·∫øng Vi·ªát.
                            """

                            prompt_tokens = self.estimate_tokens(prompt)
                            image_tokens = self.estimate_tokens(image=img_processed)
                            request_tokens = prompt_tokens + image_tokens

                            # Check if we're over budget
                            if token_budget and (
                                estimated_tokens_used + request_tokens > token_budget
                            ):
                                logging.warning(
                                    f"‚ö†Ô∏è ƒê√£ v∆∞·ª£t ng√¢n s√°ch token ({token_budget}). D·ª´ng x·ª≠ l√Ω."
                                )
                                break

                            try:
                                response = self._call_gemini_api(prompt, img_processed)

                                # C·∫≠p nh·∫≠t s·ªë tokens ƒë√£ s·ª≠ d·ª•ng
                                estimated_tokens_used += request_tokens

                                if (
                                    response
                                    and hasattr(response, "text")
                                    and response.text
                                ):
                                    all_text.append(response.text)
                                    processed_pages[page_key] = response.text
                                    success_count += 1
                                    page_processed = True

                                    # C·∫≠p nh·∫≠t thanh ti·∫øn ƒë·ªô
                                    pbar.update(1)
                                    # Th√™m th√¥ng tin token v√†o thanh ti·∫øn ƒë·ªô
                                    pbar.set_postfix(tokens=estimated_tokens_used)

                                    # L∆∞u ti·∫øn ƒë·ªô sau m·ªói trang th√†nh c√¥ng
                                    self._save_progress_with_retries(
                                        progress_file,
                                        processed_pages,
                                        estimated_tokens_used,
                                        current_page_retries,
                                    )
                                else:
                                    logging.warning(
                                        f"‚ö†Ô∏è Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ API cho trang {page_num + 1}"
                                    )
                                    retry_count += 1
                                    current_page_retries[page_key] = retry_count
                            except Exception as api_err:
                                error_msg = str(api_err)
                                if (
                                    "API_QUOTA_EXHAUSTED" in error_msg
                                    or "ALL_KEYS_EXHAUSTED" in error_msg
                                ):
                                    logging.warning(
                                        f"‚ö†Ô∏è T·∫•t c·∫£ API key ƒë√£ h·∫øt quota khi x·ª≠ l√Ω trang {page_num + 1}"
                                    )
                                    # L∆∞u ti·∫øn ƒë·ªô ƒë·ªÉ ti·∫øp t·ª•c sau
                                    self._save_progress_with_retries(
                                        progress_file,
                                        processed_pages,
                                        estimated_tokens_used,
                                        current_page_retries,
                                    )
                                    pbar.close()
                                    logging.info(
                                        "üíæ ƒê√£ l∆∞u ti·∫øn ƒë·ªô. C√≥ th·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω sau v·ªõi 'resume=True'"
                                    )
                                    return False
                                else:
                                    logging.error(
                                        f"‚ùå L·ªói API trang {page_num + 1}: {str(api_err)}"
                                    )
                                    retry_count += 1
                                    current_page_retries[page_key] = retry_count
                                    if retry_count <= max_page_retries:
                                        # Th·ª≠ ƒë·ªïi key n·∫øu c√≥ l·ªói
                                        self.rotate_api_key()
                                        time.sleep(1)  # T·∫°m d·ª´ng ng·∫Øn tr∆∞·ªõc khi th·ª≠ l·∫°i

                        except Exception as page_err:
                            logging.warning(
                                f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω trang {page_num + 1}: {str(page_err)}"
                            )
                            retry_count += 1
                            current_page_retries[page_key] = retry_count

                    # Count as failed if all retries were used up without success
                    if not page_processed:
                        failed_count += 1

            except KeyboardInterrupt:
                # ƒê√≥ng thanh ti·∫øn ƒë·ªô n·∫øu ƒëang hi·ªÉn th·ªã
                if "pbar" in locals():
                    pbar.close()

                # L∆∞u ti·∫øn ƒë·ªô tr∆∞·ªõc khi tho√°t
                logging.warning(
                    "‚ö†Ô∏è Ph√°t hi·ªán l·ªánh d·ª´ng t·ª´ ng∆∞·ªùi d√πng (Ctrl+C). ƒêang l∆∞u ti·∫øn ƒë·ªô..."
                )
                self._save_progress_with_retries(
                    progress_file,
                    processed_pages,
                    estimated_tokens_used,
                    current_page_retries,
                )
                logging.info(
                    f"üíæ ƒê√£ l∆∞u ti·∫øn ƒë·ªô ({len(processed_pages)}/{total_pages} trang). C√≥ th·ªÉ ti·∫øp t·ª•c v·ªõi resume=True"
                )

                # ƒê√≥ng PDF document tr∆∞·ªõc khi tho√°t
                if pdf_document:
                    try:
                        pdf_document.close()
                    except Exception:
                        pass

                return False  # Return False to indicate incomplete processing

            # ƒê√≥ng thanh ti·∫øn ƒë·ªô khi ho√†n t·∫•t
            pbar.close()

            # Check if we processed any pages successfully
            if not all_text:
                logging.error(
                    f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω b·∫•t k·ª≥ trang n√†o c·ªßa PDF ({failed_count}/{total_pages} l·ªói)"
                )
                return False

            total_time = time.time() - start_time
            minutes = int(total_time // 60)
            seconds = int(total_time % 60)
            logging.info(
                f"üìä K·∫øt qu·∫£ x·ª≠ l√Ω: {success_count}/{total_pages} trang th√†nh c√¥ng "
                f"({success_count / total_pages * 100:.1f}%) trong {minutes}m {seconds}s"
            )

            # Join all text and process
            try:
                logging.info("üîç ƒêang ki·ªÉm tra v√† ƒë·ªãnh d·∫°ng Markdown...")
                combined_text = "\n\n".join(all_text)

                try:
                    corrected_text = self.check_vietnamese_text(combined_text)
                except Exception as vn_err:
                    logging.warning(f"‚ö†Ô∏è L·ªói ki·ªÉm tra ti·∫øng Vi·ªát: {str(vn_err)}")
                    corrected_text = combined_text

                # Create output filename
                pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
                os.makedirs(self.output_dir, exist_ok=True)
                output_path = os.path.join(self.output_dir, f"{pdf_filename}.md")

                try:
                    cleaned_text = self.clean_markdown_content(corrected_text)
                except Exception as clean_err:
                    logging.warning(f"‚ö†Ô∏è L·ªói l√†m s·∫°ch Markdown: {str(clean_err)}")
                    cleaned_text = corrected_text

                # Save the final text
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(cleaned_text)

                logging.info(f"‚úÖ ƒê√£ l∆∞u vƒÉn b·∫£n: {output_path}")
                return output_path

            except Exception as process_err:
                logging.error(f"‚ùå L·ªói x·ª≠ l√Ω vƒÉn b·∫£n cu·ªëi c√πng: {str(process_err)}")

                # Try to save raw text if final processing fails
                try:
                    emergency_text = "\n\n".join(all_text)
                    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
                    emergency_path = os.path.join(
                        self.output_dir, f"{pdf_filename}_emergency.md"
                    )

                    with open(emergency_path, "w", encoding="utf-8") as f:
                        f.write(emergency_text)

                    logging.error(f"‚ö†Ô∏è ƒê√£ l∆∞u vƒÉn b·∫£n kh·∫©n c·∫•p: {emergency_path}")
                    return emergency_path
                except Exception as emergency_err:
                    logging.error(
                        f"‚ùå Kh√¥ng th·ªÉ l∆∞u vƒÉn b·∫£n kh·∫©n c·∫•p: {str(emergency_err)}"
                    )
                    return False

        except Exception as e:
            logging.error(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi PDF: {str(e)}")
            # Save progress on exception too
            self._save_progress_with_retries(
                progress_file,
                processed_pages,
                estimated_tokens_used,
                current_page_retries,
            )
            if "pbar" in locals():
                pbar.close()
            return False

        finally:
            # Make sure to close the PDF document to release the file handle
            if pdf_document:
                try:
                    pdf_document.close()
                except Exception as close_err:
                    logging.warning(f"‚ö†Ô∏è L·ªói khi ƒë√≥ng file PDF: {str(close_err)}")

    def cleanup(self):
        """Clean up temporary resources after processing"""
        try:
            # Clean up temp directory if it exists
            if hasattr(self, "temp_dir") and os.path.exists(self.temp_dir):
                try:
                    import shutil

                    shutil.rmtree(self.temp_dir)
                    logging.info(f"‚úÖ ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {self.temp_dir}")
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

            # Reset API rotation counters to prevent issues on next run
            if hasattr(self, "_rotation_cycle_count"):
                self._rotation_cycle_count = 0

            if hasattr(self, "_current_call_rotations"):
                self._current_call_rotations = 0

            if hasattr(self, "rotation_count"):
                self.rotation_count = 0

            if hasattr(self, "retry_count"):
                self.retry_count = 0

            # Save current state of quota tracker
            try:
                if hasattr(self, "quota_tracker") and hasattr(
                    self, "save_quota_tracker"
                ):
                    self.save_quota_tracker()
            except Exception as quota_err:
                logging.warning(f"‚ö†Ô∏è L·ªói l∆∞u tr·ªØ quota tracker: {str(quota_err)}")

            # Clean up progress files for completed documents
            progress_dir = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), "data", "progress"
            )
            if os.path.exists(progress_dir):
                # Only remove progress files older than 7 days
                cutoff = datetime.now() - timedelta(days=7)
                try:
                    for filename in os.listdir(progress_dir):
                        if filename.endswith("_progress.json"):
                            filepath = os.path.join(progress_dir, filename)
                            file_time = datetime.fromtimestamp(
                                os.path.getmtime(filepath)
                            )
                            if file_time < cutoff:
                                os.remove(filepath)
                                logging.warning(f"üßπ ƒê√£ x√≥a ti·∫øn ƒë·ªô c≈©: {filename}")
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d·ªçn d·∫πp file ti·∫øn ƒë·ªô: {str(e)}")

            logging.info("‚úÖ ƒê√£ d·ªçn d·∫πp t√†i nguy√™n t·∫°m th·ªùi")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è L·ªói khi d·ªçn d·∫πp: {str(e)}")
